{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### LLM과 RAG를 사용하여 AI 챗봇을 구현해보기"
      ],
      "metadata": {
        "id": "qfVnID_Q2vQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. 사용환경 준비\n",
        "이번 과제에서는 OpenAI 모델 혹은 Gemini 모델 중 원하시는 모델을 사용하시면 됩니다. 원하는 모델에 따른 사용 환경을 준비하겠습니다.\n",
        "\n",
        "**Yejin kang:** OpenAI를 사용하였고, 환경 변수로 키를 넣어주었기 때문에 환경 변수의 존재 여부를 확인하는 코드를 추가하였습니다."
      ],
      "metadata": {
        "id": "4jNqQv4JkqIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")  # Secrets에서 직접 가져오기\n",
        "\n",
        "if openai_api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "    print(\"OpenAI API Key가 성공적으로 설정되었습니다.\")\n",
        "else:\n",
        "    print(\"환경 변수에 OPENAI_API_KEY가 설정되어 있지 않습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV6vSJpy-4uj",
        "outputId": "9a705ea4-4e0c-4414-ec1f-962b20980fff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key가 성공적으로 설정되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. 모델 로드하기\n",
        "OpenAI 모델 혹은 Gemini 모델을 로드하여 model에 저장하세요.\n",
        "\n",
        "**Yejin Kang:** 모델은 gpt 4o를 사용하였습니다."
      ],
      "metadata": {
        "id": "6ZyWpMrtlNl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GTM4yISB_HYi",
        "outputId": "14509c51-eae9-4bde-e669-6876afa741fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.9-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.17 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.3.18)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.54.4)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (0.1.143)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (4.66.6)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.17->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.17->langchain-openai) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.17->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.17->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.17->langchain-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
            "Downloading langchain_openai-0.2.9-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-openai\n",
            "Successfully installed langchain-openai-0.2.9 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# 모델 초기화\n",
        "model = ChatOpenAI(model=\"gpt-4o\")"
      ],
      "metadata": {
        "id": "verAkykr0wim"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. 문서 로드하기\n",
        "langchain의 `PyPDFLoader` 를 이용하여 문서를 불러옵니다.\n",
        "\n",
        "문서는 아래의 문서 중 원하는 문서 중 하나를 다운로드 받으시면 됩니다.\n",
        "\n",
        "**Yejin Kang:** 문서는 인공지능산업최신동향_2024년11월호를 선택하였습니다. 3가지 예시 중 최신 인공지능 산업동향에 대한 정보가 가장 흥미로웠기 때문입니다.\n"
      ],
      "metadata": {
        "id": "s4ZUdGD7KsPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 코랩을 통해 작업해본적이 없기 때문에 기본 작업 디렉토리 확인이 필요하였습니다.\n",
        "# 현재 작업 디렉토리 확인\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAoKF77mLp8d",
        "outputId": "75ca0eac-d798-4494-d7e6-b9491b5ea21e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일을 업로드해서 사용하기 위한 코드를 추가하였습니다.\n",
        "# 아래의 코드를 실행하면 파일 업로드 창이 열려 업로드 할 파일을 선택할 수 있습니다.\n",
        "from google.colab import files\n",
        "\n",
        "# 파일 업로드\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "Jj_5evQgMHSh",
        "outputId": "deda6173-6f54-4d49-ee04-74fcaa973ad7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-33a1fb8f-4852-4890-b424-1b2df9c95852\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-33a1fb8f-4852-4890-b424-1b2df9c95852\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 인공지능산업최신동향_2024년11월호.pdf to 인공지능산업최신동향_2024년11월호.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# langchain-community를 사용하기 위해 설치를 먼저 진행하였습니다.\n",
        "# 이건 LangChain 라이브러리의 확장 모듈입니다.\n",
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HP0VLY7TNCG1",
        "outputId": "e4ebc27e-5c51-4685-e05d-771a18f96ead"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Collecting SQLAlchemy<2.0.36,>=1.4 (from langchain-community)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.1)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.7)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.17 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.18)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.17->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.7->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.7->langchain-community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.7-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: SQLAlchemy, python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "Successfully installed SQLAlchemy-2.0.35 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.7 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.6.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pypdf 설치도 함께 진행하였습니다.\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lun6fbyzNf6o",
        "outputId": "72a6751a-baaa-4c4b-c53b-f0088a45426b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# PDF 파일 경로\n",
        "file_path = \"/content/인공지능산업최신동향_2024년11월호.pdf\"\n",
        "\n",
        "# PDF 파일 로드\n",
        "loader = PyPDFLoader(file_path)\n",
        "\n",
        "# 페이지 별 문서 로드\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "CkXW9NFsLb1W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. 문서 청크로 나누기\n",
        "불러온 문서를 대상으로 아래 i, ii 청킹방법을 모두 수행하세요.\n",
        "\n",
        "청킹을 완수하면 청킹된 내용을 상위 10개까지 출력하고, 각 청킹방식과 parameter의 뜻을 markdown으로 정리해주세요. 청킹된 내용을 출력하는 이유는, 각 청킹방식에 따른 결과물을 여러분들이 눈으로 직접 확인하셨으면 하는 바입니다!"
      ],
      "metadata": {
        "id": "mkp3f3UyNzDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. CharacterTextSplitter\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",                   # 문단 구분자를 \\n\\n 기준으로 나눔\n",
        "    chunk_size=100,                     # 한 청크의 최대 길이를 설정하는 파라미터\n",
        "    chunk_overlap=10,                   # 청크 간 겹치는 길이. 문맥을 유지하기 위해 얼마나 중복해서 겹칠것인가를 지정\n",
        "    length_function=len,                # 텍스트 길이를 계산하는 함수. 기본적으로 'len' 함수를 사용한다.\n",
        "    is_separator_regex=False,           # separator를 정규식으로 처리할 지 여부\n",
        ")\n",
        "\n",
        "character_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"CharacterTextSplitter 결과 (상위 10개):\")\n",
        "for i, chunk in enumerate(character_splits[:10]):\n",
        "    print(f\"청크 {i + 1}: {chunk}\")"
      ],
      "metadata": {
        "id": "jZyQYtoxNxYM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3daf589-802f-41d7-e831-0ad858a0fdc4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharacterTextSplitter 결과 (상위 10개):\n",
            "청크 1: page_content='2024년 11월호' metadata={'source': '/content/인공지능산업최신동향_2024년11월호.pdf', 'page': 0}\n",
            "청크 2: page_content='2024년 11월호\n",
            "Ⅰ. 인공지능 산업 동향 브리프 1. 정책/법제    ▹ 미국 민권위원회, 연방정부의 얼굴인식 기술 사용에 따른 민권 영향 분석························1   ▹ 미국 백악관 예산관리국, 정부의 책임 있는 AI 조달을 위한 지침 발표·····························2   ▹ 유로폴, 법 집행에서 AI의 이점과 과제를 다룬 보고서 발간··············································3   ▹ OECD, 공공 부문의 AI 도입을 위한 G7 툴킷 발표··························································4   ▹ 세계경제포럼, 생성AI 시대의 거버넌스 프레임워크 제시····················································5  2. 기업/산업    ▹ CB인사이츠 분석 결과, 2024년 3분기 벤처 투자 31%가 AI 스타트업에 집중··············6   ▹ 메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개···································································7   ▹ 메타, 이미지와 텍스트 처리하는 첫 멀티모달 AI 모델 ‘라마 3.2’ 공개···························8   ▹ 앨런AI연구소, 벤치마크 평가에서 GPT-4o 능가하는 성능의 오픈소스 LLM ‘몰모’ 공개····9   ▹ 미스트랄AI, 온디바이스용 AI 모델 ‘레 미니스트로’ 공개·················································10   ▹ 카카오, 통합 AI 브랜드 겸 신규 AI 서비스 ‘카나나’ 공개···············································11 3. 기술/연구   ▹ 2024년 노벨 물리학상과 화학상, AI 관련 연구자들이 수상············································12   ▹ 미국 국무부, AI 연구에서 국제협력을 위한 ‘글로벌 AI 연구 의제’ 발표························13   ▹ 일본 AI안전연구소, AI 안전성에 대한 평가 관점 가이드 발간········································14   ▹ 구글 딥마인드, 반도체 칩 레이아웃 설계하는 AI 모델 ‘알파칩’ 발표·····························15   ▹ AI21 CEO, AI 에이전트에 트랜스포머 아키텍처의 대안 필요성 강조····························16    4. 인력/교육        ▹ MIT 산업성과센터, 근로자 관점에서 자동화 기술의 영향 조사········································17   ▹ 다이스 조사, AI 전문가의 73%는 2025년 중 이직 고려················································18   ▹ 가트너 예측, AI로 인해 엔지니어링 인력의 80%가 역량 향상 필요 ·····························19   ▹ 인디드 조사 결과, 생성AI가 인간 근로자 대체할 가능성은 희박·····································20    \n",
            "Ⅱ. 주요 행사  ▹NeurIPS 2024 ······················································································································21  ▹GenAI Summit Maroc 2024 ·····························································································21  ▹AI Summit Seoul 2024 ·····································································································21' metadata={'source': '/content/인공지능산업최신동향_2024년11월호.pdf', 'page': 1}\n",
            "청크 3: page_content='Ⅰ. 인공지능 산업 동향 브리프' metadata={'source': '/content/인공지능산업최신동향_2024년11월호.pdf', 'page': 2}\n",
            "청크 4: page_content='1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\n",
            "1\n",
            "미국 민권위원회, 연방정부의 얼굴인식 기술 사용에 따른 민권 영향 분석n미국 민권위원회에 따르면 연방정부와 법 집행기관에서 얼굴인식 기술이 빠르게 도입되고 있으나 이를 관리할 지침과 감독의 부재로 민권 문제를 초래할 위험 존재n미국 민권위원회는 연방정부의 책임 있는 얼굴인식 기술 사용을 위해 운영 프로토콜 개발과 실제 사용 상황의 얼굴인식 기술 평가 및 불평등 완화, 지역사회의 의견 수렴 등을 권고\n",
            "KEY Contents\n",
            "£연방정부의 얼굴인식 기술 도입에 대한 지침과 감독 부재로 민권 문제를 초래할 위험 존재n미국 민권위원회(U.S. Commission on Civil Rights)가 2024년 9월 19일 연방정부의 얼굴인식 기술 사용이 민권에 미치는 영향을 분석한 보고서를 발간∙AI 기술의 일종인 얼굴인식 기술은 연방정부와 법 집행기관에서 빠르게 도입되고 있으며, 일례로 법무부 연방수사국(FBI)은 범죄 수사 및 용의자 수색용 단서 확보를 위해 얼굴인식 기술을 가장 빈번히 사용∙그러나 얼굴인식 기술의 책임 있는 사용을 위한 연방 지침과 감독은 실제 활용 사례보다 뒤처졌으며, 현재 연방정부의 얼굴인식 기술이나 여타 AI 기술 사용을 명시적으로 규제하는 법률도 부재  n보고서에 따르면 얼굴인식 기술의 무분별한 사용은 편향, 개인정보 침해, 적법 절차의 미준수 및 차별적 영향과 같은 민권 문제를 초래할 위험 보유∙얼굴인식 기술의 정확도는 인종, 성별, 연령 등 인구통계학적 요인에 따라 달라질 수 있으며, 이는 식별 오류 및 부정확한 체포로 이어져 유색인종을 비롯한 특정 집단에 차별적 결과를 초래할 위험 존재∙정부 기관이 사전 영장이나 정당한 이유 없이 얼굴인식 기술을 광범위하게 사용할 경우 개인을 지속적으로 추적하고 감시함으로써 개인정보 보호 권리에 심각한 영향을 미칠 위험 존재∙법 집행기관의 얼굴인식 기술 사용 시 부정확한 식별 및 편향으로 인해 개인이 법의 보호를 받아 공정하고 올바르게 대우받을 권리를 침해할 가능성도 존재£민권위원회, 연방정부의 책임 있는 얼굴인식 기술 사용을 위한 권고사항 제시n민권위원회는 연방정부의 얼굴인식 기술 사용과 관련해 다음과 같은 권고사항을 제시∙국립표준기술연구소(NIST)는 정부 기관의 얼굴인식 기술 시스템 도입 시의 효과와 공평성, 정확성 평가에 사용할 수 있는 운영 테스트 프로토콜의 개발 필요∙각 연방정부 기관의 최고AI책임자는 실제 사용 상황에서 얼굴인식 기술을 평가하고 차별이나 편견으로 인한 불평등을 완화하며, 얼굴인식 기술의 사용으로 영향을 받는 지역사회의 의견을 수렴 필요∙얼굴인식 기술 제공업체는 다양한 인구통계 집단에 대한 높은 정확도를 보장하기 위해 지속적인 교육과 지원, 업데이트를 제공 필요 ☞ 출처: U.S. Commission on Civil Rights, The Civil Rights Implications of the Federal Use of Facial Recognition Technology, 2024.09.19.' metadata={'source': '/content/인공지능산업최신동향_2024년11월호.pdf', 'page': 3}\n",
            "청크 5: page_content='SPRi AI Brief |  2024-11월호\n",
            "2\n",
            "미국 백악관 예산관리국, 정부의 책임 있는 AI 조달을 위한 지침 발표n미국 백악관 예산관리국이 바이든 대통령의 AI 행정명령에 따라 연방정부의 책임 있는 AI 조달을 지원하기 위한 지침을 발표 n지침은 정부 기관의 AI 조달 시 AI의 위험과 성과를 관리할 수 있는 모범 관행의 수립 및 최상의 AI 솔루션을 사용하기 위한 공급업체 시장의 경쟁 보장, 정부 기관 간 협업을 요구  \n",
            "KEY Contents\n",
            "£백악관 예산관리국, 연방정부의 AI 조달 시 책임성을 증진하기 위한 모범 관행 제시n미국 백악관 예산관리국(OMB)이 바이든 대통령의 AI 행정명령에 따른 후속 조치로 2024년 10월 3일 ‘정부의 책임 있는 AI 조달 지침(M-24-18)’을 발표∙미국 연방정부는 2023년 1,000억 달러 이상의 IT 제품과 서비스를 구매한 미국 경제 최대 규모의 단일 구매자로서 구매력을 활용해 책임 있는 AI의 발전을 뒷받침할 계획∙이번 지침은 △AI 위험과 성과 관리 △AI 시장의 경쟁 촉진 △연방정부 전반의 협업 보장이라는 3개 전략적 목표에 대하여 권고사항을 제시n(AI 위험과 성과 관리) 예산관리국의 지침은 AI 시스템의 구축, 훈련, 배포 방식의 복잡성을 고려해 AI의 위험과 성과를 관리하기 위한 모범 관행을 다음과 같이 제시∙정부 기관의 개인정보 보호 담당자가 AI 조달 프로세스에 조기에 지속적으로 참여해 개인정보 보호 위험을 식별 및 관리하고 법률과 정책 준수를 보장∙정부 기관과 공급업체와 간 협력으로 AI 솔루션이 조달되는 시기와 해당 조달로 인해 시민 권리와 안전에 영향을 미치는 AI에 대하여 추가로 위험관리가 필요한 시점을 파악∙성과 기반의 혁신적 조달 기법을 활용해 정부 기관이 위험을 효과적으로 관리 및 완화하고 성과를 향상할 수 있도록 장려하는 한편, 정부 데이터와 지식재산권을 보호하는 방식으로 계약 조건을 협상 n(AI 시장의 경쟁 촉진) 지침은 정부 기관이 최상의 AI 솔루션을 사용할 수 있도록 공급업체 시장에서 강력한 경쟁을 보장할 것을 요구  ∙계약 요건 수립 시 공급업체 의존성을 최소화할 수 있는 인수 원칙을 적용하고, 시장 조사와 요구사항 개발, 공급업체 평가 절차에서 상호운용성과 투명성을 고려하며, 혁신적 조달 관행을 활용해 우수한 계약업체 성과와 정부 기관의 임무 성과를 보장n(연방정부 전반의 협업 보장) 빠르게 발전하는 AI 기술환경의 위험관리를 위해 AI 전문지식을 갖춘 공무원과 조달, 개인정보보호, 사이버보안 전문가를 포함하는 협업 팀을 구성해 전략적 조달을 지원  ∙각 정부 기관은 기관 간 협의회를 구성해 효과적이고 책임 있는 AI 조달을 지원하고, 협업 시 기관 목표에 가장 적합한 AI 투자 식별 및 우선순위 지정, AI 배포 역량 개발, AI 모범 활용 사례 채택 증진 등을 고려☞ 출처: The White House, FACT SHEET: OMB Issues Guidance to Advance the Responsible Acquisition of AI in Government, 2024.10.03.' metadata={'source': '/content/인공지능산업최신동향_2024년11월호.pdf', 'page': 4}\n",
            "청크 6: page_content='1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\n",
            "3\n",
            "유로폴, 법 집행에서 AI의 이점과 과제를 다룬 보고서 발간n유로폴의 보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오 분석 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유n그러나 AI 도입을 위해서는 기술적 과제 해결 및 다양한 윤리적·사회적 이슈 대응이 필요하며, EU AI 법에 부합하도록 기존 AI 시스템에 대한 평가와 수정도 필요\n",
            "KEY Contents\n",
            "£유로폴, 법 집행에서 AI 기술의 윤리적이고 투명한 구현을 위한 고려사항 제시nEU 사법기관 유로폴(Europol)이 2024년 9월 24일 법 집행에서 효과적 범죄 퇴치를 위한 AI의 활용 가능성을 탐색한 보고서를 발간∙보고서는 법 집행에서 AI 기술을 윤리적이고 투명하게 구현하기 위한 지침 역할을 하며, AI의 이점과 과제를 함께 다룸으로써 법 집행에서 AI 사용 시 윤리적 고려 사항에 대한 인식 제고를 추구n보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오 분석, 생체인식 시스템 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유∙법 집행기관은 AI 기반 데이터 분석을 활용해 범죄 활동에 대한 탐지와 대응 능력을 강화하고, AI 도구로 구조화되지 않은 데이터를 신속히 분석해 비상 상황의 의사결정을 위한 통찰력 확보 가능 ∙기계번역과 같은 AI 기반 도구는 여러 국가가 참여하는 조사에서 원활한 국제협력을 위해서도 필수적n그러나 법 집행에서 AI 도구의 효과적이고 책임 있는 활용을 위해 해결되어야 할 기술적 과제 및 다양한 윤리적·사회적 우려도 존재∙일례로 관할권 간 데이터 수집과 보관 관행의 차이에 따른 데이터셋의 편향으로 인해 AI 산출물의 무결성(無缺性)이 손상될 수 있어 표준화된 데이터 수집 규약 필요∙데이터 규모나 활용 사례의 복잡성과 관계없이 AI 도구를 효과적으로 사용하려면 다양한 데이터 규모와 운영 요구사항에 적응할 수 있는 확장성과 성능을 갖춘 AI 모델도 개발 필요∙편향, 개인정보 침해와 인권 침해와 같은 다양한 윤리적·사회적 우려도 존재하며, 이를 해소하기 위해 데이터 편향을 제거하고 공공 안전과 개인정보 간 균형을 유지하며 AI 의사 결정 과정에 대한 투명성과 책임성을 보장 필요n보고서는 2024년 8월 발효된 EU AI 법이 법 집행기관에 미칠 영향도 분석∙EU AI 법은 공공장소에서 실시간 생체인식 식별과 같은 특정 애플리케이션의 사용을 금지하고 고위험 AI 시스템에 엄격한 감독을 부과하였으나 법 집행 활동의 특수성을 고려해 일부 예외를 설정 ∙그러나 일부 예외에도 법 집행 역량 강화를 위한 AI 사용을 위해서는 기존에 도입한 AI 시스템에 대한 재평가와 수정이 필요한 만큼, 재정과 인력 측면의 상당한 부담 예상☞ 출처: Europol, AI and policing-The benefits and challenges of artificial intelligence for law enforcement, 2024.09.24.' metadata={'source': '/content/인공지능산업최신동향_2024년11월호.pdf', 'page': 5}\n",
            "청크 7: page_content='SPRi AI Brief |  2024-11월호\n",
            "4\n",
            "OECD, 공공 부문의 AI 도입을 위한 G7 툴킷 발표nOECD는 공공 부문에서 EU 및 G7 국가들의 AI 도입 모범사례와 거버넌스 프레임워크, 정책 옵션을 토대로 공공 부문의 AI 도입을 안내하는 보고서를 발표n보고서는 공공 부문의 AI 도입 시 프로토타입부터 시작해 시범 도입을 거쳐 본격적으로 구현하는 단계별 접근방식을 권고\n",
            "KEY Contents\n",
            "£OECD, G7의 사례를 토대로 공공 부문의 AI 도입을 안내하는 지침 마련nOECD가 2024년 10월 15일 안전하고 신뢰할 수 있는 AI의 원칙을 실행 가능한 정책으로 전환할 수 있도록 지원하는 ‘공공 부문의 AI를 위한 G7 툴킷’ 보고서를 발간∙OECD는 G7 회원국이 작성한 설문 응답 및 OECD와 UNESCO의 연구를 토대로 공공 부문에서 AI 활용 모범사례와 거버넌스 프레임워크, 정책 옵션과 관련된 종합적 지침 제공을 목표로 보고서를 작성nG7과 EU의 AI 도입 추세를 분석한 결과, G7 회원국과 EU는 공공 부문의 AI 도입과 관련된 국가 전략 및 정책의 개발과 구현에서 차이가 존재  ∙EU·독일·미국·영국·일본은 국가 AI 전략에 공공 부문을 포함했고 프랑스는 국가 AI 전략에서는 공공 부문을 구체적으로 다루지 않으나 공공행정 혁신기금(FTAP)을 조성하여 60개 이상의 AI 프로젝트에 투자하는 등 별도의 정책을 수립∙캐나다는 2025년 봄까지 공공 서비스를 위한 AI 전략을 개발할 계획이며, 이탈리아는 ‘공공 부문 디지털화를 위한 3개년 계획(2024~2026)’에 AI를 포함 ∙G7 회원국들은 접근방식의 차이에도 인재와 기술 개발, 조달 정책, 협력관계 구축, 윤리적이고 신뢰할 수 있으며 인간 중심적인 AI 관행 조성, 데이터 품질 보장 등에서 공통점을 보유nAI 거버넌스 프레임워크 측면에서 G7 회원국 중 미국·캐나다·프랑스와 EU는 여러 기관이 AI를 관리하는 분산형 거버넌스 구조를 채택했으며 이탈리아·독일·영국은 단일 기관이 AI를 관리하는 중앙집중형 거버넌스를 채택nG7 회원국들은 공공 부문의 운영 효율성 향상, 정책 결정 강화, 공공 서비스 개선, 정부의 투명성과 책임성 강화를 위해 AI를 활용하는 한편, 다양한 정책 옵션으로 AI 도입 시의 과제 해결을 모색∙AI 도입에 필수적인 인프라를 강화하기 위한 데이터 저장과 공유 솔루션 채택, AI에 적합한 혁신적이고 유연한 조달 절차의 수립 및 민간 파트너십 육성, 공공 부문의 AI 역량 강화, 데이터 거버넌스 프레임워크 구축 등이 대표적인 정책 옵션n보고서는 공공 부문의 AI 도입 시 각 단계를 신중히 관리하여 위험을 완화할 수 있도록, 문제를 명확히 정의하고 아이디어를 구상한 뒤 프로토타입부터 시작해 통제된 환경에서 AI를 시범 도입한 후 이를 개선해 본격적으로 구현하는 단계적 접근방식을 강조☞ 출처: OECD, G7 Toolkit for Artificial Intelligence in the Public Sector, 2024.10.15.' metadata={'source': '/content/인공지능산업최신동향_2024년11월호.pdf', 'page': 6}\n",
            "청크 8: page_content='1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\n",
            "5\n",
            "세계경제포럼, 생성AI 시대의 거버넌스 프레임워크 제시n세계경제포럼이 글로벌 정책입안자를 대상으로 생성AI의 공익적 활용과 경제·사회적 균형 달성, 위험 완화를 위한 거버넌스 프레임워크를 제안하는 백서를 발표n백서에 따르면 정부는 기존 규제를 평가해 생성AI로 인한 규제 격차를 해소하는 한편, 다양한 이해관계자 간 지식 공유를 촉진하고 미래의 AI 발전에 대비한 규제 민첩성을 갖출 필요\n",
            "KEY Contents\n",
            "£생성AI 거버넌스, 과거-현재-미래를 아우르는 프레임워크 수립 필요n세계경제포럼(WEF)이 2024년 10월 8일 세계 각국의 정책입안자를 대상으로 생성AI 거버넌스 프레임워크를 제시한 백서를 발간∙백서는 생성AI의 공익적 활용과 경제·사회적 균형 달성, 위험 완화라는 목표 달성을 위해 △과거 활용(Harness Past) △현재 구축(Build Present) △미래 계획(Plan Future)의 프레임워크를 제안n(과거 활용) 기존 규제를 활용하고 생성AI로 인한 규제 격차를 해소하는 것으로, 정부는 새로운 AI 규제나 관할 당국을 수립하기에 앞서 다음 사항을 추진할 필요∙생성AI로 인한 문제나 격차 발생에 관하여 기존 규제를 평가하고 다양한 규제 수단의 정책 목표를 고려해 규제를 조정하며, 규제 선례를 참고해 책임 할당을 명확히 하고 격차가 발견된 부분을 보완∙기존 규제 당국이 생성AI 문제를 해결할 역량이 있는지 평가하고, AI 전담 기관을 설치하여 규제 권한을 집중하는 방안의 장단점을 고려n(현재 구축) 사회 전반의 생성AI 거버넌스와 지식 공유의 증진을 의미하며, 생성AI의 거버넌스에는 정책입안자와 규제 당국 외에 산업계, 시민사회, 학계를 포함한 이해관계자 참여가 필수적∙정부는 다양한 거버넌스 수단을 활용해 사회 전반의 생성AI 거버넌스에 참여하는 각 이해관계자 집단의 고유한 문제에 대응 필요∙다양한 이해관계자 간 지식 공유를 촉진하고, 책임 있는 AI 관행으로 사회에 모범을 보일 필요성 존재n(미래 계획) 생성AI 거버넌스에 대한 민첩한 준비와 함께 국제협력을 촉진하는 것으로, 정부는 빠른 기술 발전과 한정된 자원, 글로벌 불확실성을 고려해 미래를 예견한 국가 전략을 개발하고 다음의 활동을 추진∙정부 내 AI 역량 향상과 AI 전문가 채용을 위한 투자를 시행하고 AI 전담 기관의 설립 필요성을 신중히 검토∙생성AI와 인간 간 상호작용, 생성AI와 여타 기술의 융합, 생성AI 신기능과 관련된 혁신 및 이로 인한 새로운 위험을 탐색 ∙기존 규제의 영향 평가 및 미래 AI 발전에 대비한 영향 평가로 규제 민첩성을 유지하며, 일례로 광범위한 도입에 앞서 규제 유예제도(샌드박스)를 시범 운영∙지식과 인프라 공유와 AI 안전성 연구, AI 표준의 일관성 확보를 위한 국제협력 추진☞ 출처: World Economic Forum, Governance in the Age of Generative AI: A 360° Approach for Resilient Policy and Regulation, 2024.10.08.' metadata={'source': '/content/인공지능산업최신동향_2024년11월호.pdf', 'page': 7}\n",
            "청크 9: page_content='SPRi AI Brief |  2024-11월호\n",
            "6\n",
            "CB인사이츠 분석 결과, 2024년 3분기 벤처 투자 31%가 AI 스타트업에 집중nCB인사이츠에 따르면 2024년 3분기 AI 스타트업은 전체 벤처 투자의 31%를 유치했으며, AI 스타트업의 투자금 회수 시점은 일반 기업보다 6년 빠른 것으로 확인n그러나 CB인사이츠는 투자자들의 낙관적 기대에도 불구하고 오픈AI와 같은 거대 기업도 비용 통제에 어려움을 겪고 있다며 상당수 AI 스타트업이 실패할 것으로 예상\n",
            "KEY Contents\n",
            "£AI 스타트업, 벤처 투자의 최우선 고려 대상으로 부상n글로벌 리서치 기업 CB인사이츠(CB Insights)가 2024년 10월 3일 발표한 2024년 3분기 벤처 현황 보고서에 따르면 2024년 3분기 벤처 자금의 31%가 AI 스타트업에 투자된 것으로 분석  ∙AI 스타트업은 2024년 2분기에 전체 벤처 투자의 35%를 유치하며 역대 최고 비중을 차지했으며, 3분기에도 역대 두 번째로 높은 비중을 기록∙오픈AI의 공동설립자 일리야 수츠케버(Ilya Sutskever)가 2024년 6월 설립한 스타트업 SSI(Safe Superintelligence Inc.)는 10억 달러를 유치하며 3분기 대표적인 AI 투자로 기록∙CB인사이츠가 전 세계 1만 5천 개 이상의 AI 스타트업을 추적한 결과, 전 세계 AI 스타트업의 43%가 미국 기업이며, 다음 순위는 중국이 9%, 영국이 7%, 인도와 캐나다가 각각 4%로 미국과 상당한 격차를 기록n기업가치 10억 달러 이상의 유니콘 기업은 2024년 3분기에 24개가 탄생했으며, 이중 절반 이상이 AI 기업인 것으로 확인∙범용 로봇 개발기업 스킬드AI(Skild AI), 공간지능에 특화된 월드랩스(World Labs), 법률 AI 서비스 기업 하비(Harvey) 등이 유니콘 지위를 획득nAI 스타트업은 투자금 회수(Exit) 시점도 일반 스타트업보다 훨씬 빨라 AI 기업이 엑시트하는 시점은 설립 후 7년에 불과했으나 여타 스타트업은 13년 소요되었으며, 이러한 경향은 M&A에서 가장 뚜렷해 2024년 AI 스타트업 엑시트는 대부분 M&A를 통해 달성∙대기업들은 자사 제품군에 AI 도구를 신속히 도입하고자 AI 스타트업 인수에 적극적인 행보를 보이고 있으며, 일례로 엔비디아(Nvidia)는 2024년에 AI 스타트업 3곳을 인수했고, 세일즈포스(Salesforce)는 2024년 9월 AI 스타트업 2곳을 인수n그러나 CB인사이츠는 투자자들의 낙관적 기대에도 불구하고 현재의 AI스타트업 중 상당수는 기대에 부응하지 못하고 실패하게 될 것으로 예상∙CB인사이츠는 오픈AI와 같은 거대 AI 기업조차도 수익을 내지 못해 비용을 통제해야 하는 어려움을 겪고 있다며, 오픈AI의 2024년 손실 규모가 50억 달러에 달할 것으로 전망☞ 출처 : CB Insights, State of Venture Q3’24 Report, 2024.10.03.' metadata={'source': '/content/인공지능산업최신동향_2024년11월호.pdf', 'page': 8}\n",
            "청크 10: page_content='1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\n",
            "7\n",
            "메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개n메타가 동영상 생성, 개인화 동영상 제작, 동영상 편집, 오디오 생성과 같은 기능을 지원하는 ‘메타 무비 젠’을 공개하고 2025년 중 인스타그램 등 자사 플랫폼에 통합할 계획n메타 무비 젠은 인간 선호도 평가에서 런웨이의 젠 3, 오픈AI의 소라, 클링 1.5와 같은 경쟁 동영상 AI 모델보다 더 높은 점수를 기록\n",
            "KEY Contents\n",
            "£메타, 동영상 제작과 편집, 오디오 생성을 지원하는 메타 무비 젠을 공개 n메타(Meta)가 2024년 10월 4일 텍스트 입력을 통해 고해상도 동영상을 생성하는 AI 도구 ‘메타 무비 젠(Meta Movie Gen)’을 공개∙메타는 크리에이터와 영화 제작자 등 소수의 외부 파트너에게 메타 무비 젠을 우선 제공 후 피드백을 반영해 기능을 개선할 계획으로, 단독 서비스로 출시하는 대신 2025년 중 인스타그램(Instagram)과 같은 자사 소셜미디어 플랫폼에 통합하여 제공할 방침n메타 무비 젠은 △동영상 생성 △개인화 동영상 생성 △동영상 편집 △오디오 생성의 4가지 기능을 지원∙(동영상 생성) 300억 개 매개변수의 AI 모델을 통해 초당 16프레임의 속도로 1,080p 해상도의 최대 16초 길이 동영상 생성을 지원∙(개인화 동영상 생성) 사용자가 자신이나 타인의 이미지와 텍스트를 입력해 원래 인물의 고유한 특징을 반영한 개인화 동영상을 제작 가능∙(동영상 편집) 특정 요소의 추가나 제거, 변경과 같은 부분적 수정 및 동영상 배경 또는 스타일 변경과 같은 광범위한 수정도 지원∙(오디오 생성) 130억 개 매개변수의 오디오 생성 모델을 통합해 동영상과 텍스트 프롬프트 기반으로 최대 45초 길이의 배경음, 음향 효과 등 고품질 오디오를 생성£메타 무비 젠, 인간 선호도 평가에서 오픈AI의 소라 능가n메타 무비 젠은 인간 선호도 평가에서 런웨이(Runway)의 젠(Gen) 3, 오픈AI의 소라(Sora)를 비롯한 경쟁 동영상 생성AI 모델보다 더 높은 점수를 기록∙메타 무비 젠과 경쟁 모델에 대하여 세 명의 인간 평가자가 점수를 매겨 비교 후 순승률(Net Win Rate)*을 계산한 결과, 메타 무비 젠은 젠 3와 소라, 클링(Kling) 1.5를 모두 능가* 두 모델(A와 B)에 대하여 3명의 인간 평가자가 A 선호 시 +1점, 동점이면 0점, B 선호 시 –1점을 매기는 식으로 계산해 승률(-100%~100% 값)을 구하며, 승률이 양수면 A 모델 선호, 음수면 B 모델 선호를 의미     \n",
            "<메타 무비 젠과 경쟁 AI 모델의 인간 선호도 평가 승률>\n",
            "☞ 출처: Meta, How Meta Movie Gen could usher in a new AI-enabled era for content creators, 2024.10.04.' metadata={'source': '/content/인공지능산업최신동향_2024년11월호.pdf', 'page': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "# 2. RecursiveCharacterTextSplitter 설정\n",
        "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,            # 한 청크의 최대 길이\n",
        "    chunk_overlap=10,          # 청크 간 겹치는 길이\n",
        "    length_function=len,       # 텍스트 길이를 계산하는 함수\n",
        "    is_separator_regex=False,  # separator를 정규식으로 처리할지 여부\n",
        ")\n",
        "\n",
        "# split_documents 사용\n",
        "recursive_splits = recursive_text_splitter.split_documents(docs)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"\\nRecursiveCharacterTextSplitter 결과 (상위 10개):\")\n",
        "for i, chunk in enumerate(recursive_splits[:10]):\n",
        "    print(f\"청크 {i + 1}: {chunk.page_content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxdJQmH2_kVm",
        "outputId": "6e46609e-2c93-449c-fc3f-fda3655f17f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RecursiveCharacterTextSplitter 결과 (상위 10개):\n",
            "청크 1: 2024년 11월호\n",
            "청크 2: 2024년 11월호\n",
            "청크 3: Ⅰ. 인공지능 산업 동향 브리프 1. 정책/법제    ▹ 미국 민권위원회, 연방정부의 얼굴인식 기술 사용에 따른 민권 영향 분석························1\n",
            "청크 4: ▹ 미국 백악관 예산관리국, 정부의 책임 있는 AI 조달을 위한 지침 발표·····························2   ▹ 유로폴, 법 집행에서 AI의 이점과\n",
            "청크 5: AI의 이점과 과제를 다룬 보고서 발간··············································3   ▹ OECD, 공공 부문의 AI 도입을 위한 G7\n",
            "청크 6: 도입을 위한 G7 툴킷 발표··························································4   ▹ 세계경제포럼, 생성AI 시대의\n",
            "청크 7: 생성AI 시대의 거버넌스 프레임워크 제시····················································5  2. 기업/산업    ▹ CB인사이츠\n",
            "청크 8: ▹ CB인사이츠 분석 결과, 2024년 3분기 벤처 투자 31%가 AI 스타트업에 집중··············6   ▹ 메타, 동영상 생성AI 도구 ‘메타 무비 젠’\n",
            "청크 9: ‘메타 무비 젠’ 공개···································································7   ▹ 메타, 이미지와 텍스트\n",
            "청크 10: 이미지와 텍스트 처리하는 첫 멀티모달 AI 모델 ‘라마 3.2’ 공개···························8   ▹ 앨런AI연구소, 벤치마크 평가에서 GPT-4o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 방식의 차이는 문서를 청크로 나누는 기준과 과정의 차이에서 발생한다.\n",
        "1. CharacterTextSplitter\n",
        "- 동작 방식:\n",
        "  - 지정된 separator를 기준으로 텍스트를 나눈 후, 나뉜"
      ],
      "metadata": {
        "id": "9Or3SB9Wr3VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. 벡터 임베딩 생성\n",
        "OpenAI 모델의 경우 OpenAIEmbeddings , Gemini 모델의 경우GoogleGenerativeAIEmbeddings 를 이용해 텍스트를 벡터로 변환할 벡터 임베딩을 생성하세요."
      ],
      "metadata": {
        "id": "Te-2Gm7LsNyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# OpenAI 임베딩 모델 초기화\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
      ],
      "metadata": {
        "id": "APWEJBbisNMR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. 벡터 스토어 생성\n",
        "앞서 만든 벡터 임베딩과 청크된 문서를 활용하여 `FAISS` 벡터 스토어를 생성하세요."
      ],
      "metadata": {
        "id": "RAx0YZCQsah5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJA0yhz4sZX7",
        "outputId": "85a91ba1-e0f3-4053-88bf-b4d274ed5199"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "\n",
        "vector_store = FAISS.from_documents(documents=splits, embedding=embeddings)"
      ],
      "metadata": {
        "id": "j8_on293s5H0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. FAISS를 Retriever로 변환\n",
        "RAG 체인에서 사용할 수 있도록 FAISS를 retriever로 변환하세요."
      ],
      "metadata": {
        "id": "4BVwzbRTs8XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
      ],
      "metadata": {
        "id": "8G-XII1ftCsy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. 프롬프트 템플릿 정의\n",
        "프롬프트 템플릿을 정의하세요."
      ],
      "metadata": {
        "id": "IE__bCg8tnDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "contextual_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the question using only the following context.\"),\n",
        "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
        "])"
      ],
      "metadata": {
        "id": "AtOdGWZBtlF_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. RAG 체인 구성\n",
        "LangChain의 모델과 프롬프트를 연결하여 RAG 체인을 구성하세요."
      ],
      "metadata": {
        "id": "nqVLS0-RtyFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "contextual_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the question using only the following context.\"),\n",
        "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
        "])\n",
        "\n",
        "class DebugPassThrough(RunnablePassthrough):\n",
        "    def invoke(self, *args, **kwargs):\n",
        "        output = super().invoke(*args, **kwargs)\n",
        "        print(\"Debug Output:\", output)\n",
        "        return output\n",
        "# 문서 리스트를 텍스트로 변환하는 단계 추가\n",
        "class ContextToText(RunnablePassthrough):\n",
        "    def invoke(self, inputs, config=None, **kwargs):  # config 인수 추가\n",
        "        # context의 각 문서를 문자열로 결합\n",
        "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
        "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
        "\n",
        "# RAG 체인에서 각 단계마다 DebugPassThrough 추가\n",
        "rag_chain_debug = {\n",
        "    \"context\": retriever,                    # 컨텍스트를 가져오는 retriever\n",
        "    \"question\": DebugPassThrough()        # 사용자 질문이 그대로 전달되는지 확인하는 passthrough\n",
        "}  | DebugPassThrough() | ContextToText()|   contextual_prompt | model\n"
      ],
      "metadata": {
        "id": "lH5u3kSPtxVz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. 챗봇 구동 확인\n",
        "질문에 응답하는 챗봇을 구동하여 질문해보세요.\n",
        "\n",
        "같은 질문을 일반 chat gpt 혹은 Gemini에 질문해보고 답변을 비교해보고, 왜 RAG이 필요한지 간단히 markdown으로 서술해주세요."
      ],
      "metadata": {
        "id": "dEt9QFlkt5-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "질문 내용: (1)메타 무비 젠이 뭐야, (2)이거 언제 공개 된 내용이야\n",
        "\n",
        "이전에는 GhatGPT가 학습되지 않은 내용에 대해서는 답변할수 없거나, 이상한 말을 지어서 한것 같았는데 요즘 GPT는 웹 서치를 해서 정보를 제공해서 그런지 이런 동향 같은 정보는 RAG에 입력하지 않고도 일반 chatGPT가 검색으로 안내해주는 것 같습니다. 오히려 RAG 로 학습시켜서 질문하였을 때 상대적으로 덜 자세한 정보를 안내받았습니다. RAG로 개인적인 일정내역 같은 것을 학습시켜서 AI 비서 역할로 쓰면 좋을 것 같습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "vLlUGMkTwSTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:  # 무한 루프 조건 추가\n",
        "    print(\"========================\")\n",
        "    query = input(\"질문을 입력하세요: \")\n",
        "\n",
        "    if query.lower() in [\"exit\", \"quit\"]:  # 종료 조건\n",
        "        print(\"프로그램을 종료합니다.\")\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        response = rag_chain_debug.invoke(query)  # 메서드 호출\n",
        "        print(\"Final Response:\")\n",
        "        print(response.content)\n",
        "    except AttributeError as e:\n",
        "        print(f\"오류 발생: response 객체에 'content' 속성이 없습니다. ({e})\")\n",
        "    except Exception as e:\n",
        "        print(f\"예기치 못한 오류 발생: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK1_l10muCV8",
        "outputId": "f91d548d-e53e-4181-a765-4fa39c054973"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================\n",
            "질문을 입력하세요: \b메타가 공개한 동영상 생성 도구에 대해 알려줘\n",
            "Debug Output: \b메타가 공개한 동영상 생성 도구에 대해 알려줘\n",
            "Debug Output: {'context': [Document(metadata={'source': '/content/인공지능산업최신동향_2024년11월호.pdf', 'page': 9}, page_content='메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개n메타가 동영상 생성, 개인화 동영상 제작, 동영상 편집, 오디오 생성과 같은 기능을 지원하는 ‘메타 무비 젠’을 공개하고')], 'question': '\\x08메타가 공개한 동영상 생성 도구에 대해 알려줘'}\n",
            "Final Response:\n",
            "메타가 공개한 동영상 생성 도구는 '메타 무비 젠'입니다. 이 도구는 동영상 생성, 개인화 동영상 제작, 동영상 편집, 오디오 생성 등의 기능을 지원합니다. 이를 통해 사용자는 보다 쉽게 다양한 동영상을 제작하고 편집할 수 있습니다.\n",
            "========================\n",
            "질문을 입력하세요: 메타 무비 젠이 뭐야\n",
            "Debug Output: 메타 무비 젠이 뭐야\n",
            "Debug Output: {'context': [Document(metadata={'source': '/content/인공지능산업최신동향_2024년11월호.pdf', 'page': 1}, page_content='‘메타 무비 젠’ 공개···································································7   ▹ 메타, 이미지와 텍스트')], 'question': '메타 무비 젠이 뭐야'}\n",
            "Final Response:\n",
            "메타 무비 젠은 메타(구 페이스북)에서 개발한 새로운 콘텐츠 생성 플랫폼입니다. 이 플랫폼은 이미지와 텍스트 데이터를 활용하여 사용자에게 다양한 형태의 콘텐츠를 제공합니다. 메타 무비 젠은 사용자가 창의적인 콘텐츠를 쉽게 제작할 수 있도록 도와주는 도구로, 특히 이미지와 텍스트를 결합한 혁신적인 콘텐츠 생성에 중점을 두고 있습니다.\n",
            "========================\n",
            "질문을 입력하세요: 이거 언제 공개 된 내용이야\n",
            "Debug Output: 이거 언제 공개 된 내용이야\n",
            "Debug Output: {'context': [Document(metadata={'source': '/content/인공지능산업최신동향_2024년11월호.pdf', 'page': 1}, page_content='‘메타 무비 젠’ 공개···································································7   ▹ 메타, 이미지와 텍스트')], 'question': '이거 언제 공개 된 내용이야'}\n",
            "Final Response:\n",
            "제공된 정보에 따르면, ‘메타 무비 젠’이 공개된 시점은 명확하게 언급되어 있지 않습니다. 추가적인 세부사항이 필요합니다.\n",
            "========================\n",
            "질문을 입력하세요: exit\n",
            "프로그램을 종료합니다.\n"
          ]
        }
      ]
    }
  ]
}